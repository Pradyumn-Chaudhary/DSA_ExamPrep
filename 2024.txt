Question 1 (a) -> Difference between space and time complexity? Explain omega notation with the help of examples and compare it with the other popular notation.
Ans:
Difference between Space and Time Complexity:
Time complexity is the amount of time required to execute an algorithm. 
Space Complexity is the amount of space required to save the problem, its products, and output.

Omega notation (Ω) basically describes the best-case scenario. It is the opposite of big O notation. It is a formal way to represent the lower bound of an algorithm's running time. It measures the best amount of time an algorithm can possibly take to complete, or best-case time complexity.
Example:
Linear Search: If the element is at the first position, it’s Ω(1) because only one check is needed.
Binary Search: If the element is at the middle for the array, it's Ω(1) because only one check is needed.

Comparison of Ω with Other Notations:
Big-O (O): Represents the worst-case complexity, giving an upper bound.
Theta (Θ): Represents the average-case complexity, giving a tight bound (both lower and upper).
Omega (Ω): Represents the best-case, giving a lower bound.

Relation:
O(f(n)) ≥ Θ(f(n)) ≥ Ω(f(n))


(b) -> Compare the time and space complexity of the following sorting algorithms. Bubble Sort, Insertion Sort, Merge Sort, Selection Sort, Shell Sort, Quick Sort, Heap Sort.
Ans:
| Algorithm          | Best Time Complexity | Worst Time Complexity | Average Time Complexity | Space Complexity   |
|--------------------|----------------------|-----------------------|-------------------------|--------------------|
| Bubble Sort        | O(n)                 | O(n²)                 | O(n²)                   | O(1)               |
| Insertion Sort     | O(n)                 | O(n²)                 | O(n²)                   | O(1)               |
| Selection Sort     | O(n²)                | O(n²)                 | O(n²)                   | O(1)               |
| Merge Sort         | O(n log n)           | O(n log n)            | O(n log n)              | O(n)               |
| Shell Sort         | O(n log n)           | O(n²)                 | O(n^(3/2))Dep on gap seq| O(1)               |
| Quick Sort         | O(n log n)           | O(n²)                 | O(n log n)              | O(log n)           |
| Heap Sort          | O(n log n)           | O(n log n)            | O(n log n)              | O(1)               |



Question 2 (a) -> A stack is used to implement recursion within a program.Explain how the calls are tracked and finally programs exit from recursion? Explain with the example of Fibonacci series.
Ans:
How the Stack Works:
When a recursive function is called, the function call is pushed onto the call stack.
Each time a function calls itself, a new stack frame is created containing its local variables, parameters, and return address.
Once the base case is reached, the recursion unwinds, and each function call is popped from the stack.
Example: Fibonacci Series Using Recursion:
#include <stdio.h>
// Recursive function to calculate Fibonacci
int fibonacci(int n) {
    if (n <= 1) {
        return n;  // Base case: fib(0) = 0, fib(1) = 1
    }
    return fibonacci(n - 1) + fibonacci(n - 2);  // Recursive case
}

int main() {
    int n = 3;
    printf("Fibonacci of %d is %d\n", n, fibonacci(n));
    return 0;
}

Step-by-Step Stack Table:
|------------------------------------------------------------------|
| Stack Depth | Call               | Action                        |
|-------------|--------------------|-------------------------------|
| 1           | fibonacci(3)       | Calls fibonacci(2)            |
| 2           | fibonacci(2)       | Calls fibonacci(1)            |
| 3           | fibonacci(1)       | Base case reached (returns 1) |
| 2           | fibonacci(2)       | Calls fibonacci(0)            |
| 3           | fibonacci(0)       | Base case reached (returns 0) |
| 2           | fibonacci(2)       | Returns 1 (1 + 0)             |
| 1           | fibonacci(3)       | Calls fibonacci(1)            |
| 2           | fibonacci(1)       | Base case reached (returns 1) |
| 1           | fibonacci(3)       | Returns 2 (1 + 1)             |
|------------------------------------------------------------------|


(b) -> Write a C function for insertion SOrt. Sort the following list using insertion sort.
50,30,10,70,40,20,60
Ans:
#include <stdio.h>

// Function to perform Insertion Sort
void insertionSort(int arr[], int n) {
    int i, key, j;
    for (i = 1; i < n; i++) {
        key = arr[i];  // The element to be inserted into the sorted portion
        j = i - 1;

        // Move elements of arr[0..i-1] that are greater than key to one position ahead
        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j];
            j = j - 1;
        }
        arr[j + 1] = key;  // Insert the key into its correct position
    }
}

// Function to print the array
void printArray(int arr[], int n) {
    for (int i = 0; i < n; i++) {
        printf("%d ", arr[i]);
    }
    printf("\n");
}

int main() {
    int arr[] = {50, 30, 10, 70, 40, 20, 60};
    int n = sizeof(arr) / sizeof(arr[0]);

    printf("Original Array:\n");
    printArray(arr, n);

    // Perform Insertion Sort
    insertionSort(arr, n);

    printf("Sorted Array:\n");
    printArray(arr, n);

    return 0;
}

Sorted List:
10, 20, 30, 40, 50, 60, 70
Time Complexity:
Best Case: O(n) when the array is already sorted.
Worst Case: O(n²) when the array is in reverse order.
Average Case: O(n²)
Space Complexity:
O(1) as it is an in-place sorting algorithm.


Question 3 (a) -> Describe the doubly Linked List with advantages and disadvantages. Write a function to delete a node from a circular doubly linked list with the header node.
Ans:
Doubly Linked List (DLL):
Definition: A doubly linked list is a type of linked list where each node contains three parts:
Data: Holds the actual data.
Next Pointer: Points to the next node in the list.
Previous Pointer: Points to the previous node in the list.
Structure:
Head node (first node) points to NULL (in the case of empty list) or to the first data node.
The last node's next pointer points back to the head (if circular), or to NULL in a non-circular list.

Advantages of Doubly Linked List:
Bidirectional Traversal: Can be traversed in both forward and backward directions.
Efficient Deletion: Deletion of a node is more efficient compared to singly linked lists because you have direct access to the previous node.
Flexible: Easier to implement operations like reverse traversal, insertion, and deletion at both ends.

Disadvantages of Doubly Linked List:
Extra Memory: Requires extra memory for storing an additional pointer (previous) for each node.
Complex Implementation: Requires more complex logic for insertion, deletion, and traversal, especially when compared to singly linked lists.
Overhead: Slightly more overhead in terms of memory and time due to the additional pointer.

Function to Delete a Node from a Circular Doubly Linked List (with Header Node):
Assumptions:

The list is circular, so the last node's next points to the header node.
We have a header node that doesn't store any data but points to the first node.

void deleteNode(struct Node* header, int value) {
    struct Node* temp = header->next;

    // Traverse the list to find the node with the given value
    while (temp != header) {
        if (temp->data == value) {
            // Adjust the previous and next pointers of surrounding nodes
            temp->prev->next = temp->next;
            temp->next->prev = temp->prev;

            // Free the node
            free(temp);
            return;
        }
        temp = temp->next;
    }

    printf("Node with value %d not found!\n", value);
}


(b) ->  What is the advantage of circular queue over ordinary queue? Write a C program to stimulate the working of circular queue of integers using array.Provide the following operation:
(i) insert
(ii) delete
(iii) display
Ans:
Advantages of Circular Queue over Ordinary Queue:
Efficient Use of Space:
In a normal queue, when elements are deleted from the front, there is no way to reuse the empty spaces at the front, even if there is room in the rear. This causes unused space.
In a circular queue, the front and rear pointers can wrap around and use the empty spaces created by deletions, thus optimizing space usage.
No Wasted Space:
Circular queue ensures continuous use of space, unlike a regular queue that may have wasted space when elements are deleted, but not inserted at the front.


#include <stdio.h>
#define SIZE 5  // Maximum size of the queue
// Circular Queue structure
struct CircularQueue {
    int arr[SIZE];
    int front, rear;
};

// Function to initialize the queue
void initializeQueue(struct CircularQueue *queue) {
    queue->front = -1;
    queue->rear = -1;
}

// Check if the queue is full
int isFull(struct CircularQueue *queue) {
    return (queue->rear + 1) % SIZE == queue->front;
}

// Check if the queue is empty
int isEmpty(struct CircularQueue *queue) {
    return queue->front == -1;
}

// Insert an element into the queue
void insert(struct CircularQueue *queue, int value) {
    if (isFull(queue)) {
        printf("Queue is full! Cannot insert %d\n", value);
        return;
    }

    if (queue->front == -1) {  // If queue is empty
        queue->front = 0;
    }

    queue->rear = (queue->rear + 1) % SIZE;  // Circular increment
    queue->arr[queue->rear] = value;
    printf("Inserted %d into the queue.\n", value);
}

// Delete an element from the queue
void delete(struct CircularQueue *queue) {
    if (isEmpty(queue)) {
        printf("Queue is empty! Cannot delete.\n");
        return;
    }

    printf("Deleted %d from the queue.\n", queue->arr[queue->front]);
    
    if (queue->front == queue->rear) {  // Only one element left
        queue->front = queue->rear = -1;  // Queue becomes empty
    } else {
        queue->front = (queue->front + 1) % SIZE;  // Circular increment
    }
}

// Display the elements of the queue
void display(struct CircularQueue *queue) {
    if (isEmpty(queue)) {
        printf("Queue is empty!\n");
        return;
    }

    printf("Queue elements: ");
    int i = queue->front;
    while (i != queue->rear) {
        printf("%d ", queue->arr[i]);
        i = (i + 1) % SIZE;
    }
    printf("%d\n", queue->arr[queue->rear]);  // Display last element
}

int main() {
    struct CircularQueue queue;
    initializeQueue(&queue);

    // Perform some operations
    insert(&queue, 10);
    insert(&queue, 20);
    insert(&queue, 30);
    insert(&queue, 40);
    insert(&queue, 50);  // Queue is now full

    display(&queue);

    delete(&queue);  // Delete 10
    insert(&queue, 60);  // Insert 60 after deletion

    display(&queue);

    delete(&queue);  // Delete 20
    delete(&queue);  // Delete 30

    display(&queue);

    return 0;
}

Time Complexity:
Insert: O(1) (constant time)
Delete: O(1) (constant time)
Display: O(n) (where n is the number of elements)
Space Complexity:
O(n) (for storing n elements in the queue)


Question 4 (a) -> What do you mean by B-Tree? Draw a B-tree of order 5 for the following sequence of keys:
10,20,50,60,40,80,100,70,130,90,30,120,140,25,35,160,180
Ans:
B-Tree:
A B-Tree is a self-balancing search tree in which:
Each node can have multiple children.
Nodes store multiple keys in sorted order.
The keys in each node are stored in a way that maintains the balance of the tree.
It is a generalization of a binary search tree (BST) that allows nodes to have more than two children.

Properties of a B-Tree of Order m:
Each node can have at most m-1 keys and m children.
Every internal node (except root) must have at least ⌈m/2⌉ children.
The root can have at least two children if it is not a leaf.
All leaf nodes are at the same level.

      ___[40, 70, 100]___
     /       |    \       \
    /        |     |       \
  [25]__ [50, 60] [80, 90] [130, 140, 160, 180]
 /      \
[10, 20] [30, 35]
  

Question 5 (a) -> Define Graph.
For the given graph,show the adjacency matrix and adjacency list representation of the graph
B is connected to A,C,E
A is connected to B,C,D
C is connected to D,F
D is connected to E,F
             B
      / /    |    \
      A -- C     E
         \   |   /  \
            D----F
Ans:
Definition of a Graph:
A graph is a collection of nodes (or vertices) and edges that connect pairs of nodes. It is a mathematical structure used to model pairwise relations between objects. A graph can be represented in different ways, such as:

Adjacency Matrix
Adjacency List

Adjacency Matrix Representation:
In an adjacency matrix, a square matrix is used to represent a graph. The matrix element A[i][j] is 1 if there is an edge between node i and node j, otherwise 0.

Adjacency List Representation:
In an adjacency list, each node has a list of all the nodes it is directly connected to. It's a more space-efficient representation than the adjacency matrix.

The adjacency matrix for the graph is:
    A  B  C  D  E  F
A [ 0, 1, 1, 1, 0, 0 ]
B [ 1, 0, 1, 0, 1, 0 ]
C [ 1, 1, 0, 1, 0, 1 ]
D [ 1, 0, 1, 0, 1, 1 ]
E [ 0, 1, 0, 1, 0, 1 ]
F [ 0, 0, 1, 1, 1, 0 ]

The adjacency list for the graph is:
A: B, C, D
B: A, C, E
C: A, B, D, F
D: A, C, E, F
E: B, D, F
F: C, D, E


(b) -> What are the methods used for traversing a graph?
Explain any one with example and write C function for the same.
Ans:
Methods for Traversing a Graph:
There are two primary methods used for traversing a graph:
Depth-First Search (DFS)
Breadth-First Search (BFS)
These methods are used to explore all nodes and edges in a graph, but they differ in the order in which nodes are visited.
1. Depth-First Search (DFS):
Description: DFS explores as far down a branch of the graph as possible before backtracking. It uses a stack (either implicitly with recursion or explicitly) to track nodes.
Process:
Start from a source node, visit it, and mark it as visited.
Explore all adjacent unvisited nodes recursively.
Once all adjacent nodes are visited, backtrack to explore the next unvisited nodes.
DFS can be implemented using two approaches:

Recursive DFS (uses system stack)
Iterative DFS (uses an explicit stack)
Example of DFS Traversal:
Consider the following graph:
         A
        / \
       B   C
      / \   \
     D   E   F
Starting from vertex A, DFS will visit A, then B, then D, backtrack to B, visit E, backtrack to A, and visit C, then F.
The DFS traversal order: A -> B -> D -> E -> C -> F
C Function for DFS (Recursive Approach):

#include <stdio.h>
#include <stdlib.h>

#define MAX_VERTICES 6  // Adjust this based on the number of vertices

// Graph structure
typedef struct Graph {
    int adjMatrix[MAX_VERTICES][MAX_VERTICES];
    int visited[MAX_VERTICES];
} Graph;

// Function to initialize the graph
void initializeGraph(Graph *graph) {
    for (int i = 0; i < MAX_VERTICES; i++) {
        graph->visited[i] = 0;
        for (int j = 0; j < MAX_VERTICES; j++) {
            graph->adjMatrix[i][j] = 0;  // Initialize adjacency matrix to 0 (no edges)
        }
    }
}

// Function to add an edge between two nodes
void addEdge(Graph *graph, int src, int dest) {
    graph->adjMatrix[src][dest] = 1;
    graph->adjMatrix[dest][src] = 1;  // Since it's an undirected graph
}

// DFS function (recursive)
void DFS(Graph *graph, int vertex) {
    // Mark the current node as visited and print it
    graph->visited[vertex] = 1;
    printf("%c ", vertex + 'A');  // Print the node (A = 0, B = 1, ..., F = 5)

    // Recursively visit all the adjacent vertices that are not visited
    for (int i = 0; i < MAX_VERTICES; i++) {
        if (graph->adjMatrix[vertex][i] == 1 && !graph->visited[i]) {
            DFS(graph, i);
        }
    }
}

int main() {
    Graph graph;
    initializeGraph(&graph);

    // Add edges (for the given graph A-B, A-C, B-D, B-E, C-F)
    addEdge(&graph, 0, 1);  // A-B
    addEdge(&graph, 0, 2);  // A-C
    addEdge(&graph, 1, 3);  // B-D
    addEdge(&graph, 1, 4);  // B-E
    addEdge(&graph, 2, 5);  // C-F

    // Perform DFS starting from vertex 0 (A)
    printf("DFS Traversal Order: ");
    DFS(&graph, 0);
    printf("\n");

    return 0;
}

Time Complexity of DFS:
Time Complexity: O(V + E), where V is the number of vertices and E is the number of edges in the graph. This is because each vertex and edge is explored once.
Space Complexity: O(V), where V is the number of vertices, due to the recursion stack or the visited array


Question 6 (a) -> What do you mean by hash functions? Explain common hashing functions along with all address calculation techniques.
Ans:
Hash Functions:
A hash function maps data to a fixed-size value, called a hash code. It's used in hash tables for efficient data retrieval.

Common Hashing Functions:
1. Division Method: `h(k) = k % m`
    Simple, but poor performance if `m` is not a prime.
2. Multiplicative Method: `h(k) = floor(m * (k * A % 1))`
   - Better distribution using an irrational constant `A`.
3. Mid Square Method: Square the key and take the middle digits.
4. Folding Method: Split the key, sum parts, and apply modulo.
5. Cryptographic Hash Functions: e.g., SHA-256, used for security.

Address Calculation Techniques:**
1. Linear Probin*: `h'(k) = (h(k) + i) % m` (sequential checking).
2. Quadratic Probing: `h'(k) = (h(k) + i^2) % m` (quadratic steps).
3. Double Hashing: `h'(k) = (h(k) + i * h2(k)) % m` (second hash function).

Example (Linear Probing):
For keys `25, 35, 45, 55, 65` with a table size `10`:
- `h(25) = 5`, `h(35) = 5` → Collision, try `h(35) = 6`, etc.

(b) -> What is collision? What are the methods to resolves collision? Explain linear probing with an example.
Ans:
### **Collision in Hashing:**
A **collision** occurs when two different keys produce the same hash value and hence try to occupy the same index in the hash table. Since hash tables can only store one element per index, a collision needs to be resolved to ensure all elements are stored correctly.

### **Methods to Resolve Collisions:**
1. **Linear Probing**: Search for the next available slot linearly.
2. **Quadratic Probing**: Use a quadratic function to find the next available slot.
3. **Double Hashing**: Use a second hash function to calculate the step size for resolving collisions.
4. **Chaining**: Store multiple elements at the same index using a linked list.

### **Linear Probing:**
- **Method**: When a collision occurs at index `i`, check the next slot `(i + 1) % m`, and keep checking until an empty slot is found.
- **Formula**: `h'(k) = (h(k) + i) % m`, where `i` is the number of attempts.

### **Example of Linear Probing:**
Consider a hash table of size 5 and we are inserting the following keys: `10, 20, 30, 40, 50`.

- **Hash Function**: `h(k) = k % 5`.

1. **Insert 10**:  
   `h(10) = 10 % 5 = 0`.  
   Insert 10 at index `0`.

2. **Insert 20**:  
   `h(20) = 20 % 5 = 0`.  
   Collision occurs at index `0`.  
   Apply **Linear Probing**: `(0 + 1) % 5 = 1`.  
   Insert 20 at index `1`.

3. **Insert 30**:  
   `h(30) = 30 % 5 = 0`.  
   Collision at index `0`.  
   Apply **Linear Probing**: `(0 + 1) % 5 = 1`.  
   Collision at index `1`.  
   Apply **Linear Probing**: `(0 + 2) % 5 = 2`.  
   Insert 30 at index `2`.

4. **Insert 40**:  
   `h(40) = 40 % 5 = 0`.  
   Collision at index `0`.  
   Apply **Linear Probing**: `(0 + 1) % 5 = 1`.  
   Collision at index `1`.  
   Apply **Linear Probing**: `(0 + 2) % 5 = 2`.  
   Collision at index `2`.  
   Apply **Linear Probing**: `(0 + 3) % 5 = 3`.  
   Insert 40 at index `3`.

5. **Insert 50**:  
   `h(50) = 50 % 5 = 0`.  
   Collision at index `0`.  
   Apply **Linear Probing**: `(0 + 1) % 5 = 1`.  
   Collision at index `1`.  
   Apply **Linear Probing**: `(0 + 2) % 5 = 2`.  
   Collision at index `2`.  
   Apply **Linear Probing**: `(0 + 3) % 5 = 3`.  
   Collision at index `3`.  
   Apply **Linear Probing**: `(0 + 4) % 5 = 4`.  
   Insert 50 at index `4`.

### **Final Hash Table:**
```
Index 0: 10
Index 1: 20
Index 2: 30
Index 3: 40
Index 4: 50
```

### **Advantages of Linear Probing**:
- Simple and easy to implement.
- Efficient for small tables with few collisions.

### **Disadvantages of Linear Probing**:
- **Clustering**: When many collisions happen, consecutive slots become filled, increasing the time for future insertions and lookups.


Question 7 -> Write a short note on:
(a) De Queue
(b) Priority Queue
(c)AVL tree
(d)Complete Binary Tree
Ans:
### **De Queue (Double-Ended Queue):**
- **Definition**: A **dequeue** (double-ended queue) is a linear data structure that allows insertion and deletion of elements from both ends (front and rear).
- **Operations**: 
  - **Insert Front**: Insert an element at the front.
  - **Insert Rear**: Insert an element at the rear.
  - **Delete Front**: Remove an element from the front.
  - **Delete Rear**: Remove an element from the rear.
- **Applications**: Used in scenarios like sliding window problems, job scheduling, etc.

---

### **Priority Queue:**
- **Definition**: A **priority queue** is a data structure where each element is assigned a priority. Elements with higher priority are dequeued before those with lower priority, regardless of their insertion order.
- **Types**: 
  - **Max Priority Queue**: The element with the highest priority is at the front.
  - **Min Priority Queue**: The element with the lowest priority is at the front.
- **Applications**: Used in algorithms like **Dijkstra's shortest path**, **Huffman coding**, and in scheduling tasks.

---

### **AVL Tree:**
- **Definition**: An **AVL tree** is a self-balancing binary search tree where the difference in heights of the left and right subtrees of any node (called the **balance factor**) is at most 1.
- **Balance Factor**: For a node, the balance factor = height of left subtree - height of right subtree.
- **Rebalancing**: If the balance factor exceeds 1 or -1, rotations (left or right) are performed to restore balance.
- **Applications**: Used in situations where fast searching, insertion, and deletion are needed with a guaranteed time complexity of **O(log n)**.

---

### **Complete Binary Tree:**
- **Definition**: A **complete binary tree** is a binary tree in which every level, except possibly the last, is completely filled, and all nodes are as far left as possible.
- **Properties**:
  - All levels are filled except for the last one.
  - The nodes in the last level are filled from left to right.
- **Applications**: Used in **heap data structures** (max-heap and min-heap) for efficient insertions and deletions.